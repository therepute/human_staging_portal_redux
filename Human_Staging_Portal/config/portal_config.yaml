# Human Staging Portal Configuration

# Database Configuration
database:
  provider: "supabase"
  staging_table: "soup_dedupe"  # Articles ready for human extraction (lowercase)
  destination_table: "the_soups"  # Final extracted content
  supabase_url: "https://nuumgwdbiifwsurioavm.supabase.co"
  # SUPABASE_ANON_KEY loaded from environment

# Query Filters for Staging Table (Soup_Dedupe)
staging_filters:
  extraction_path: 2  # Human extraction required
  wf_pre_check_complete: true  # Articles that passed pre-check
  status: "pending"  # Ready for processing

# Task Assignment Configuration
task_assignment:
  max_concurrent_tasks: 5  # Per scraper
  assignment_timeout_minutes: 30
  priority_refresh_interval: 300  # 5 minutes

# Priority Scoring System
priority_scoring:
  client_priority_base: 1000      # Base points for client articles
  source_priority_base: 500       # Base points for high-value sources  
  publication_tier_multipliers:
    tier_1: 400  # Top tier publications
    tier_2: 300  # Major publications
    tier_3: 200  # Regional publications
    tier_4: 100  # Local/niche publications
    tier_5: 50   # Low priority publications
  time_factors:
    urgent_hours: 2     # Articles < 2 hours old get bonus
    urgent_bonus: 200   # Bonus points for urgent articles
    decay_hours: 24     # Start applying age penalty after 24 hours
    max_decay: 300      # Maximum penalty for old articles
  headline_relevance_multiplier: 100  # Base points for relevance
  retry_penalty: 50   # Penalty per retry attempt

# Domain Safety and Cooldowns
domain_safety:
  default_cooldown_seconds: 60
  default_concurrent_limit: 3
  publisher_rules:
    "wsj.com":
      cooldown_seconds: 180  # 3 minutes
      concurrent_limit: 1
    "nytimes.com":
      cooldown_seconds: 120  # 2 minutes
      concurrent_limit: 2
    "reuters.com":
      cooldown_seconds: 90
      concurrent_limit: 2
    "bloomberg.com":
      cooldown_seconds: 90
      concurrent_limit: 2
    "techcrunch.com":
      cooldown_seconds: 20
      concurrent_limit: 3
    "cnn.com":
      cooldown_seconds: 30
      concurrent_limit: 2
    "bbc.com":
      cooldown_seconds: 30
      concurrent_limit: 2

# API Configuration
api:
  host: "0.0.0.0"
  port: 8001
  cors_origins: ["*"]  # Allow all origins for Retool
  rate_limit_per_minute: 100

# Logging Configuration
logging:
  level: "INFO"
  log_file: "logs/portal.log"
  max_file_size_mb: 10
  backup_count: 5
  log_to_console: true

# Retool Integration
retool:
  webhook_secret: "human_portal_2024"  # For secure webhooks
  task_assignment_endpoint: "/api/tasks/next"
  submission_endpoint: "/api/tasks/submit"
  failure_endpoint: "/api/tasks/fail"
# Performance Settings
performance:
  database_connection_pool_size: 10
  task_batch_size: 50
  cache_ttl_seconds: 300
  health_check_interval: 60